{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 3.7 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from python-Levenshtein) (49.6.0.post20210108)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp36-cp36m-linux_x86_64.whl size=82218 sha256=424b5109ab6396490557b5818364f0d31cdd238544716353be3255a27722ba9d\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/4a/a4/bf/d761b0899395c75fa76d003d607b3869ee47f5035b8afc30a2\n",
      "Successfully built python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein\n",
      "Successfully installed python-Levenshtein-0.12.2\n",
      "Requirement already satisfied: gensim in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (4.0.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gensim) (5.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gensim) (1.5.3)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gensim) (0.8)\n"
     ]
    }
   ],
   "source": [
    "# ! pip install imbalanced-learn\n",
    "# ! pip install lime\n",
    "# ! pip install textblob\n",
    "# ! pip install contractions\n",
    "# ! pip install spacy\n",
    "# ! python -m spacy download en_core_web_sm\n",
    "! pip install python-Levenshtein\n",
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "import pickle \n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# import lime\n",
    "# import lime.lime_tabular\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, plot_confusion_matrix\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleaned data\n",
    "combine title and body of text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aita_w2v = pd.read_csv('data/aita_w2v.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75267,) (761,)\n"
     ]
    }
   ],
   "source": [
    "X = aita_w2v['combo_clean']\n",
    "y = aita_w2v['is_asshole']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.01, random_state=21)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 20795, 0: 55233})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create tokenized list, detect unigrams, bigrams, trigrams\n",
    "### 2.2 Train word2vec model on list of list of unigrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 26s, sys: 1.85 s, total: 16min 27s\n",
      "Wall time: 7min 45s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# lst_corpus = [post.split() for post in X_train]\n",
    "# bigrams_detector = gensim.models.Phrases(lst_corpus, min_count=5, threshold=10)\n",
    "# trigrams_detector = gensim.models.Phrases(bigrams_detector[lst_corpus], min_count=5, threshold=10)\n",
    "# lst_corpus = list(bigrams_detector[lst_corpus])\n",
    "# lst_corpus = list(trigrams_detector[lst_corpus])\n",
    "# nlp = gensim.models.word2vec.Word2Vec(lst_corpus, vector_size=200, window=4, min_count=3, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 7.98 ms, total: 12.7 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# ## tokenize text\n",
    "# tokenizer = kprocessing.text.Tokenizer(lower=True, split=' ', oov_token=\"NaN\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "# tokenizer.fit_on_texts(lst_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.99 s, sys: 52 ms, total: 10 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# nlp.save(\"models/nlp.model\")\n",
    "# bigrams_detector.save('models/bigrams_detector.pkl')\n",
    "# trigrams_detector.save('models/trigrams_detector.pkl')\n",
    "# with open('models/tokenizer.pkl', 'wb') as f:\n",
    "#     pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = Word2Vec.load(\"models/nlp.model\")\n",
    "bigrams_detector = gensim.models.Phrases.load(\"models/bigrams_detector.pkl\")\n",
    "trigrams_detector = gensim.models.Phrases.load(\"models/trigrams_detector.pkl\")\n",
    "with open('models/tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('upset', 0.8202987313270569),\n",
       " ('mad', 0.7894049286842346),\n",
       " ('pissed_off', 0.765598714351654),\n",
       " ('irate', 0.7474846243858337),\n",
       " ('frustrated', 0.741222083568573),\n",
       " ('annoyed', 0.7384644150733948),\n",
       " ('irritated', 0.7150590419769287),\n",
       " ('snippy', 0.7134074568748474),\n",
       " ('super_pissed_off', 0.7115201354026794),\n",
       " ('snappy', 0.7107669711112976)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_.wv.most_similar('angry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect bigrams and trigrams from unigram lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 536 ms, total: 1min 38s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lst_corpus = [post.split() for post in X_train]\n",
    "lst_corpus = list(bigrams_detector[lst_corpus])\n",
    "lst_corpus = list(trigrams_detector[lst_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use fitted dectors on test set\n",
    "lst_corpus_test = [post.split() for post in X_test]\n",
    "lst_corpus_test = list(bigrams_detector[lst_corpus_test])\n",
    "lst_corpus_test = list(trigrams_detector[lst_corpus_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 tokenize text, create padded sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 s, sys: 40 ms, total: 10.4 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dic_vocabulary = tokenizer.word_index\n",
    "## create sequence\n",
    "lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)\n",
    "## padding sequence\n",
    "X_train_p = kprocessing.sequence.pad_sequences(lst_text2seq, maxlen=400, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 121 ms, sys: 0 ns, total: 121 ms\n",
      "Wall time: 121 ms\n"
     ]
    }
   ],
   "source": [
    "## text to sequence with the fitted tokenizer\n",
    "lst_text2seq_test = tokenizer.texts_to_sequences(lst_corpus_test)\n",
    "## padding sequence\n",
    "X_test_p = kprocessing.sequence.pad_sequences(lst_text2seq_test, maxlen=400, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75267 75267\n",
      "761 761\n"
     ]
    }
   ],
   "source": [
    "print(len(lst_corpus),len(X_train_p))\n",
    "print(len(lst_corpus_test),len(X_test_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 261 ms, sys: 40 ms, total: 301 ms\n",
      "Wall time: 300 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "embeddings = np.zeros((len(dic_vocabulary)+1, 200))\n",
    "for word,idx in dic_vocabulary.items():\n",
    "    ## update the row with vector\n",
    "    try:\n",
    "        embeddings[idx] =  nlp.wv[word]\n",
    "    ## if word not in model then skip and the row stays all 0s\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 400, 200)     29763600    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "permute_4 (Permute)             (None, 200, 400)     0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 200, 400)     160400      permute_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention (Permute)             (None, 400, 200)     0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 400, 200)     0           embedding_4[0][0]                \n",
      "                                                                 attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 400, 100)     100400      multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 100)          60400       bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64)           6464        bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 2)            130         dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 30,091,394\n",
      "Trainable params: 327,794\n",
      "Non-trainable params: 29,763,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "## code attention layer\n",
    "def attention_layer(inputs, neurons):\n",
    "    x = layers.Permute((2,1))(inputs)\n",
    "    x = layers.Dense(neurons, activation=\"softmax\")(x)\n",
    "    x = layers.Permute((2,1), name=\"attention\")(x)\n",
    "    x = layers.multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "## input\n",
    "x_in = layers.Input(shape=(400,))\n",
    "## embedding\n",
    "x = layers.Embedding(input_dim=embeddings.shape[0],  \n",
    "                     output_dim=embeddings.shape[1], \n",
    "                     weights=[embeddings],\n",
    "                     input_length=400, trainable=False)(x_in)\n",
    "## apply attention\n",
    "x = attention_layer(x, neurons=400)\n",
    "## 2 layers of bidirectional lstm\n",
    "x = layers.Bidirectional(layers.LSTM(units=50, dropout=0.2, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(units=50, dropout=0.2))(x)\n",
    "## final dense layers\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "y_out = layers.Dense(2, activation='softmax')(x)\n",
    "## compile\n",
    "model = models.Model(x_in, y_out)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy',precision_m,precision_m,recall_m])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75267, 400), 75267)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_p.shape,y_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## encode y\n",
    "dic_y_mapping = {n:label for n,label in enumerate(np.unique(y_train))}\n",
    "inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "y_train_i = np.array([inverse_dic[y] for y in y_train])\n",
    "\n",
    "## train\n",
    "training = model.fit(x=X_train_p, y=y_train_i, batch_size=200, \n",
    "                     epochs=10, shuffle=True, verbose=0, \n",
    "                     validation_split=0.3)\n",
    "\n",
    "# save model and architecture to single file\n",
    "training.save(\"models/keras_trained.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot loss and accuracy\n",
    "metrics = [k for k in tr\n",
    "           aining.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
    "ax[0].set(title=\"Training\")\n",
    "ax11 = ax[0].twinx()\n",
    "ax[0].plot(training.history['loss'], color='black')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "    ax11.plot(training.history[metric], label=metric)\n",
    "ax11.set_ylabel(\"Score\", color='steelblue')\n",
    "ax11.legend()\n",
    "ax[1].set(title=\"Validation\")\n",
    "ax22 = ax[1].twinx()\n",
    "ax[1].plot(training.history['val_loss'], color='black')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss', color='black')\n",
    "for metric in metrics:\n",
    "     ax22.plot(training.history['val_'+metric], label=metric)\n",
    "ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "predicted_prob = model.predict(X_test_p)\n",
    "predicted = [dic_y_mapping[np.argmax(pred)] for pred in predicted_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc_stop(s):\n",
    "    s = re.sub('[%s]' % re.escape(string.punctuation), '', s)\n",
    "    s = re.sub('[‘’“”…]', '', s)\n",
    "    s = re.sub('\\w*\\d\\w*', '', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Average the word vectors for a set of words\n",
    "    \"\"\"\n",
    "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed)\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index_to_key)  # words known to the model\n",
    "    stop = set(stopwords.words('english')).union(['aita','wibta'])\n",
    "    for word in words:\n",
    "        if word in index2word_set and word not in stop: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec,model.wv[word])\n",
    "    feature_vec = np.divide(feature_vec, nwords)\n",
    "    return feature_vec\n",
    "def get_avg_feature_vecs(texts, model, num_features):\n",
    "    \"\"\"\n",
    "    Calculate average feature vectors for all reviews\n",
    "    \"\"\"\n",
    "    feature_vecs = np.zeros((len(texts),num_features), dtype='float32')  # pre-initialize (for speed)\n",
    "    for ix, text in enumerate(texts):\n",
    "        feature_vecs[ix] = make_feature_vec(text, model, num_features)\n",
    "    return feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_vecs = get_avg_feature_vecs(aita_w2v['tokens'], model, 100)\n",
    "# feature_vecs_df = pd.DataFrame(feature_vecs, columns=[f'w2v_{i}' for i in range(1, 101)])\n",
    "# aita_w2v_1 = pd.concat([aita_w2v.reset_index(drop=True),feature_vecs_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aita_w2v_1.to_csv('data/aita_w2v_1.csv', index=False)\n",
    "# aita_w2v_1 = pd.read_csv('data/aita_w2v_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zealand', 0.9204254150390625),\n",
       " ('orleans', 0.8691218495368958),\n",
       " ('jersey', 0.744202733039856),\n",
       " ('mexico', 0.7416518330574036),\n",
       " ('england', 0.7132971286773682),\n",
       " ('hires', 0.7053131461143494),\n",
       " ('hampshire', 0.6276334524154663),\n",
       " ('boston', 0.6179364323616028),\n",
       " ('wales', 0.6043822765350342),\n",
       " ('city', 0.5941430926322937)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.wv.most_similar(positive=['york'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Oversample minority class with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 54680, 1: 54680}), Counter({1: 553, 0: 553}))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment analysis + vectors\n",
    "X_train_smote, y_train_smote = SMOTE().fit_resample(X_train, y_train)\n",
    "X_test_smote, y_test_smote = SMOTE().fit_resample(X_test, y_test)\n",
    "Counter(y_train_smote), Counter(y_test_smote)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
