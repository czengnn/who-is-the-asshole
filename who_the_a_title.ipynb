{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import pickle \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA, IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>is_asshole</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I write an explanation in til and come off a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>throw my parent donut away</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I tell a goth girl she look like a clown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>argument I have with another redditor in rhimym</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>have a disagreement about le miserable with ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97493</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>for tell my sister she be be a spoiled brat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97494</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>for tell my husband to f off after he relent...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97495</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>for attempt to keep my student out of adult ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97496</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>if I leave my brother fate up to the state</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97497</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>for rock the boat at work because my mentor ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97498 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       title_polarity  title_subjectivity  \\\n",
       "0                 0.0                 0.0   \n",
       "1                 0.0                 0.0   \n",
       "2                 0.0                 0.0   \n",
       "3                 0.0                 0.0   \n",
       "4                 0.0                 0.0   \n",
       "...               ...                 ...   \n",
       "97493             0.0                 0.0   \n",
       "97494             0.0                 0.0   \n",
       "97495             0.1                 0.3   \n",
       "97496             0.0                 0.0   \n",
       "97497             0.0                 0.0   \n",
       "\n",
       "                                             title_clean  is_asshole  \n",
       "0        I write an explanation in til and come off a...           1  \n",
       "1                             throw my parent donut away           1  \n",
       "2               I tell a goth girl she look like a clown           0  \n",
       "3        argument I have with another redditor in rhimym           1  \n",
       "4        have a disagreement about le miserable with ...           1  \n",
       "...                                                  ...         ...  \n",
       "97493        for tell my sister she be be a spoiled brat           0  \n",
       "97494    for tell my husband to f off after he relent...           0  \n",
       "97495    for attempt to keep my student out of adult ...           0  \n",
       "97496         if I leave my brother fate up to the state           0  \n",
       "97497    for rock the boat at work because my mentor ...           0  \n",
       "\n",
       "[97498 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aita_2 = pd.read_csv('data/aita_save2.csv')\n",
    "aita = aita_2.loc[:,['title_polarity','title_subjectivity','title_clean','is_asshole']]\n",
    "aita.dropna(inplace=True)\n",
    "aita.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Save features X and labels y, train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = aita.drop('is_asshole', axis=1)\n",
    "y = aita['is_asshole']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.01, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96523, 3), (975, 3))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Instantiate CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "vec = CountVectorizer(stop_words=stop, min_df=25, max_df=0.8, ngram_range=(1, 2))\n",
    "cv = vec.fit(X_train['title_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save fitted CountVectorizer to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(cv, open('models/cv_fit_train.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load fitted CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = pickle.load(open('models/cv_fit_train.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Transform train and test data into document-term-matrix with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_train = cv.transform(X_train['title_clean'])\n",
    "cv_test = cv.transform(X_test['title_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_cols = cv.get_feature_names()\n",
    "dtm_train = pd.DataFrame(cv_train.toarray(), columns=cv_cols)\n",
    "dtm_test = pd.DataFrame(cv_test.toarray(), columns=cv_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "friend        16032\n",
       "want          12914\n",
       "tell          11576\n",
       "get            7805\n",
       "girlfriend     6115\n",
       "ask            5764\n",
       "go             5602\n",
       "boyfriend      5065\n",
       "sister         4131\n",
       "mom            3904\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_count = dtm_train.sum(axis=0)\n",
    "train_word_count.sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "friend        167\n",
       "want          144\n",
       "tell          103\n",
       "get            78\n",
       "girlfriend     69\n",
       "ask            56\n",
       "go             53\n",
       "sister         49\n",
       "boyfriend      46\n",
       "give           44\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_count = dtm_test.sum(axis=0)\n",
    "test_word_count.sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96523, 7870), (975, 7870))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ((96480, 44051), (975, 44051))\n",
    "dtm_train.shape, dtm_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save / Load Document Term Matrix to / from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtm_train.to_csv('data/dtm_train.csv', index=False)\n",
    "# dtm_test.to_csv('data/dtm_test.csv', index=False)\n",
    "\n",
    "# dtm_train = pd.read_csv('data/dtm_train.csv')\n",
    "# dtm_test = pd.read_csv('data/dtm_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = IncrementalPCA(n_components=800, batch_size=850)\n",
    "pca.fit(dtm_train)\n",
    "print(pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Scree plot, check number of components is appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.title('PCA Scree Plot')\n",
    "# plt.axvline(linewidth=4, color='r', linestyle = '--', x=10, ymin=0, ymax=1)\n",
    "evr = pca.explained_variance_ratio_\n",
    "cvr = np.cumsum(pca.explained_variance_ratio_)\n",
    "pca_df = pd.DataFrame()\n",
    "pca_df['Cumulative Variance Ratio'] = cvr\n",
    "pca_df['Explained Variance Ratio'] = evr\n",
    "# display(pca_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save / Load fitted PCA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pca, open('models/pca_title.sav', 'wb'))\n",
    "# pca = pickle.load(open('models/pca.sav', 'rb'))\n",
    "# pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b - Transform train/test Document-Term-matrix with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_train_pca = pca.transform(dtm_train)\n",
    "dtm_test_pca = pca.transform(dtm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build DataFrame of PCA Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['PC_' + str(i) for i in range(1, pca.get_params()['n_components']+1)]\n",
    "dtm_train_pca_df = pd.DataFrame(dtm_train_pca, columns=col_names)\n",
    "dtm_test_pca_df = pd.DataFrame(dtm_test_pca, columns=col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine PCA components and sentiment analysis scores into one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_post_dtm_pca = pd.concat([X_train[['body_polarity','body_subjectivity']].reset_index(drop=True), dtm_train_pca_df], axis=1)\n",
    "X_test_post_dtm_pca = pd.concat([X_test[['body_polarity','body_subjectivity']].reset_index(drop=True),dtm_test_pca_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_polarity</th>\n",
       "      <th>body_subjectivity</th>\n",
       "      <th>PC_1</th>\n",
       "      <th>PC_2</th>\n",
       "      <th>PC_3</th>\n",
       "      <th>PC_4</th>\n",
       "      <th>PC_5</th>\n",
       "      <th>PC_6</th>\n",
       "      <th>PC_7</th>\n",
       "      <th>PC_8</th>\n",
       "      <th>...</th>\n",
       "      <th>PC_791</th>\n",
       "      <th>PC_792</th>\n",
       "      <th>PC_793</th>\n",
       "      <th>PC_794</th>\n",
       "      <th>PC_795</th>\n",
       "      <th>PC_796</th>\n",
       "      <th>PC_797</th>\n",
       "      <th>PC_798</th>\n",
       "      <th>PC_799</th>\n",
       "      <th>PC_800</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.089698</td>\n",
       "      <td>0.382755</td>\n",
       "      <td>6.721062</td>\n",
       "      <td>3.945255</td>\n",
       "      <td>-3.471649</td>\n",
       "      <td>-2.540817</td>\n",
       "      <td>-1.755770</td>\n",
       "      <td>-1.530562</td>\n",
       "      <td>-4.131972</td>\n",
       "      <td>3.995945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090375</td>\n",
       "      <td>-0.634156</td>\n",
       "      <td>-0.356446</td>\n",
       "      <td>0.235280</td>\n",
       "      <td>0.338331</td>\n",
       "      <td>-0.506310</td>\n",
       "      <td>0.407767</td>\n",
       "      <td>-0.156681</td>\n",
       "      <td>0.328248</td>\n",
       "      <td>-0.305217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.140000</td>\n",
       "      <td>0.601585</td>\n",
       "      <td>5.761561</td>\n",
       "      <td>0.596863</td>\n",
       "      <td>-2.237171</td>\n",
       "      <td>-3.185759</td>\n",
       "      <td>-1.131884</td>\n",
       "      <td>-2.262600</td>\n",
       "      <td>-2.888938</td>\n",
       "      <td>0.091418</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.521899</td>\n",
       "      <td>-0.763363</td>\n",
       "      <td>0.620522</td>\n",
       "      <td>0.318312</td>\n",
       "      <td>0.675178</td>\n",
       "      <td>0.760419</td>\n",
       "      <td>0.822378</td>\n",
       "      <td>-0.192563</td>\n",
       "      <td>0.465362</td>\n",
       "      <td>1.029007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034758</td>\n",
       "      <td>0.575265</td>\n",
       "      <td>1.966241</td>\n",
       "      <td>-1.712645</td>\n",
       "      <td>-0.909865</td>\n",
       "      <td>0.264539</td>\n",
       "      <td>0.921549</td>\n",
       "      <td>5.895036</td>\n",
       "      <td>1.228376</td>\n",
       "      <td>-1.104059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016981</td>\n",
       "      <td>-0.175593</td>\n",
       "      <td>0.031395</td>\n",
       "      <td>0.089073</td>\n",
       "      <td>-0.180081</td>\n",
       "      <td>0.025352</td>\n",
       "      <td>0.025359</td>\n",
       "      <td>-0.034695</td>\n",
       "      <td>0.128525</td>\n",
       "      <td>0.134961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.009470</td>\n",
       "      <td>0.545676</td>\n",
       "      <td>-0.266427</td>\n",
       "      <td>0.267081</td>\n",
       "      <td>2.156305</td>\n",
       "      <td>-2.304990</td>\n",
       "      <td>-2.705240</td>\n",
       "      <td>-2.705795</td>\n",
       "      <td>-0.498179</td>\n",
       "      <td>0.245619</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.440284</td>\n",
       "      <td>-0.017129</td>\n",
       "      <td>0.046089</td>\n",
       "      <td>-0.495012</td>\n",
       "      <td>-0.651864</td>\n",
       "      <td>0.408607</td>\n",
       "      <td>0.301299</td>\n",
       "      <td>0.130943</td>\n",
       "      <td>-0.954164</td>\n",
       "      <td>0.605056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.196644</td>\n",
       "      <td>0.494792</td>\n",
       "      <td>2.398380</td>\n",
       "      <td>-0.615288</td>\n",
       "      <td>-0.211554</td>\n",
       "      <td>-3.215119</td>\n",
       "      <td>-1.241756</td>\n",
       "      <td>1.164878</td>\n",
       "      <td>4.107588</td>\n",
       "      <td>2.698780</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076834</td>\n",
       "      <td>0.109758</td>\n",
       "      <td>-0.102025</td>\n",
       "      <td>-0.278899</td>\n",
       "      <td>0.029033</td>\n",
       "      <td>-0.506665</td>\n",
       "      <td>0.143349</td>\n",
       "      <td>-0.365991</td>\n",
       "      <td>-0.151719</td>\n",
       "      <td>0.129302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 802 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_polarity  body_subjectivity      PC_1      PC_2      PC_3      PC_4  \\\n",
       "0       0.089698           0.382755  6.721062  3.945255 -3.471649 -2.540817   \n",
       "1      -0.140000           0.601585  5.761561  0.596863 -2.237171 -3.185759   \n",
       "2       0.034758           0.575265  1.966241 -1.712645 -0.909865  0.264539   \n",
       "3      -0.009470           0.545676 -0.266427  0.267081  2.156305 -2.304990   \n",
       "4       0.196644           0.494792  2.398380 -0.615288 -0.211554 -3.215119   \n",
       "\n",
       "       PC_5      PC_6      PC_7      PC_8  ...    PC_791    PC_792    PC_793  \\\n",
       "0 -1.755770 -1.530562 -4.131972  3.995945  ...  0.090375 -0.634156 -0.356446   \n",
       "1 -1.131884 -2.262600 -2.888938  0.091418  ... -0.521899 -0.763363  0.620522   \n",
       "2  0.921549  5.895036  1.228376 -1.104059  ... -0.016981 -0.175593  0.031395   \n",
       "3 -2.705240 -2.705795 -0.498179  0.245619  ... -0.440284 -0.017129  0.046089   \n",
       "4 -1.241756  1.164878  4.107588  2.698780  ... -0.076834  0.109758 -0.102025   \n",
       "\n",
       "     PC_794    PC_795    PC_796    PC_797    PC_798    PC_799    PC_800  \n",
       "0  0.235280  0.338331 -0.506310  0.407767 -0.156681  0.328248 -0.305217  \n",
       "1  0.318312  0.675178  0.760419  0.822378 -0.192563  0.465362  1.029007  \n",
       "2  0.089073 -0.180081  0.025352  0.025359 -0.034695  0.128525  0.134961  \n",
       "3 -0.495012 -0.651864  0.408607  0.301299  0.130943 -0.954164  0.605056  \n",
       "4 -0.278899  0.029033 -0.506665  0.143349 -0.365991 -0.151719  0.129302  \n",
       "\n",
       "[5 rows x 802 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_post_dtm_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96480, 802) (96480,)\n",
      "(975, 802) (975,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_post_dtm_pca.shape, y_train.shape)\n",
    "print(X_test_post_dtm_pca.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Random Forest Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a Vanilla Random Forest, no tuning hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_post_dtm_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01932008, 0.01416853, 0.00824279, 0.01418704, 0.0134882 ])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rfc, X_train_post_dtm_pca, y_train, cv=5, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy = 0.7271794871794872\n"
     ]
    }
   ],
   "source": [
    "print(f'Testing Accuracy = {rfc.score(X_test_post_dtm_pca, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70299     0]\n",
      " [    0 26181]]\n",
      "F1 Score = 1.0\n",
      "Accuracy Score = 1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = rfc.predict(X_train_post_dtm_pca)\n",
    "print(confusion_matrix(y_train,y_pred_train))\n",
    "# print(classification_report(y_train,y_pred_train))\n",
    "print(f'F1 Score = {f1_score(y_train,y_pred_train)}')\n",
    "print(f'Accuracy Score = {accuracy_score(y_train,y_pred_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[707   3]\n",
      " [263   2]]\n",
      "F1 Score = 0.014814814814814815\n",
      "Accuracy Score = 0.7271794871794872\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = rfc.predict(X_test_post_dtm_pca)\n",
    "print(confusion_matrix(y_test,y_pred_test))\n",
    "# print(classification_report(y_test,y_pred_test))\n",
    "print(f'F1 Score = {f1_score(y_test,y_pred_test)}')\n",
    "print(f'Accuracy Score = {accuracy_score(y_test,y_pred_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b weighted random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc2 = RandomForestClassifier(class_weight=\"balanced\")\n",
    "rfc2.fit(X_train_post_dtm_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70299     0]\n",
      " [    0 26181]]\n",
      "F1 Score = 1.0\n",
      "Accuracy Score = 1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred_train_2 = rfc2.predict(X_train_post_dtm_pca)\n",
    "print(confusion_matrix(y_train,y_pred_train_2))\n",
    "# print(classification_report(y_train,y_pred_train_2))\n",
    "print(f'F1 Score = {f1_score(y_train,y_pred_train_2)}')\n",
    "print(f'Accuracy Score = {accuracy_score(y_train,y_pred_train_2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[710   0]\n",
      " [264   1]]\n",
      "F1 Score = 0.0075187969924812035\n",
      "Accuracy Score = 0.7292307692307692\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_2 = rfc2.predict(X_test_post_dtm_pca)\n",
    "print(confusion_matrix(y_test,y_pred_test_2))\n",
    "# print(classification_report(y_test,y_pred_test_2))\n",
    "print(f'F1 Score = {f1_score(y_test,y_pred_test_2)}')\n",
    "print(f'Accuracy Score = {accuracy_score(y_test,y_pred_test_2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(class_weight='balanced')\n",
    "lr.fit(X_train_post_dtm_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41623 28676]\n",
      " [ 9777 16404]]\n",
      "F1 Score = 0.460392079819256\n",
      "Accuracy Score = 0.6014407131011609\n"
     ]
    }
   ],
   "source": [
    "y_pred_train_lr = lr.predict(X_train_post_dtm_pca)\n",
    "print(confusion_matrix(y_train,y_pred_train_lr))\n",
    "# print(classification_report(y_train,y_pred_train_lr))\n",
    "print(f'F1 Score = {f1_score(y_train,y_pred_train_lr)}')\n",
    "print(f'Accuracy Score = {accuracy_score(y_train,y_pred_train_lr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[415 295]\n",
      " [104 161]]\n",
      "F1 Score = 0.4466019417475728\n",
      "Accuracy Score = 0.5907692307692308\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_lr = lr.predict(X_test_post_dtm_pca)\n",
    "print(confusion_matrix(y_test,y_pred_test_lr))\n",
    "# print(classification_report(y_test,y_pred_test_lr))\n",
    "print(f'F1 Score = {f1_score(y_test,y_pred_test_lr)}')\n",
    "print(f'Accuracy Score = {accuracy_score(y_test,y_pred_test_lr)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(dtm_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46942 23357]\n",
      " [12704 13477]]\n",
      "F1 Score = 0.4277394271205268\n",
      "Accuracy Score = 0.626233416252073\n"
     ]
    }
   ],
   "source": [
    "y_pred_train_nb = nb.predict(dtm_train)\n",
    "print(confusion_matrix(y_train,y_pred_train_nb))\n",
    "# print(classification_report(y_train,y_pred_train_nb))\n",
    "print(f'F1 Score = {f1_score(y_train,y_pred_train_nb)}')\n",
    "print(f'Accuracy Score = {accuracy_score(y_train,y_pred_train_nb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[462 248]\n",
      " [139 126]]\n",
      "F1 Score = 0.39436619718309857\n",
      "Accuracy Score = 0.6030769230769231\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_nb = nb.predict(dtm_test)\n",
    "print(confusion_matrix(y_test,y_pred_test_nb))\n",
    "# print(classification_report(y_test,y_pred_test_nb))\n",
    "print(f'F1 Score = {f1_score(y_test,y_pred_test_nb)}')\n",
    "print(f'Accuracy Score = {accuracy_score(y_test,y_pred_test_nb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
